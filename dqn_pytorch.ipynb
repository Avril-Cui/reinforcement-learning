{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GYM CartPole\n",
    "- Rewards\n",
    "    - +1 for every incremental timestep\n",
    "    - Env terminate if pole falls over or cart moves more than 2.4 units away from center\n",
    "- Performance (value)\n",
    "    - Higher value if scenarios run for longer duration, accumulating larger return\n",
    "- Input\n",
    "    - (position, velocity, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Syntax\n",
    "- namedtuple: access elements in tuple with \"keys\"; for example, transition.state=0\n",
    "- deque: efficient append and pop, fixed-size buffer (useful for replay memory in RL, where older experiences are dorpped as new ones are added, keeping a manageable memory size)\n",
    "- random.sample(array, n): randomly sample an array of length n from array\n",
    "- mask: A tensor of Boolean values (True or False) where each position indicates whether the corresponding element in another tensor should be selected or ignored\n",
    "- torch.cat() concatenates the list of tensors into a single tensor along the first dimension (batch dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas\n",
    "#### Replay Memory\n",
    "During each step of an episode, an agent generates a tuple of experience, often represented as $(\\text{state}, \\text{action}, \\text{next\\_state}, \\text{reward})$. Replay memory stores these experiences in a buffer, allowing the agent to access and learn from past actions, even if they no longer represent the current state.\n",
    "#### Neural Net\n",
    "Minimize:\n",
    "$$\\delta = Q(s,a)-r*\\gamma*\\max_{a'}Q(s',a')$$\n",
    "Use Huber loss, which acts like the mean squared error when the error is small, but like the mean absolute error when the error is large; more robust to outliers\n",
    "$$L(\\delta) = \\begin{cases}\n",
    "\\frac{1}{2}*\\delta^2 & \\text{ for } |\\delta|\\leq 1 \\\\\n",
    "|\\delta|-\\frac{1}{2} & \\text{ otherwise }\n",
    "\\end{cases}$$\n",
    "#### Policy Net\n",
    "During training, the policy_net is updated after each step or batch to minimize the difference between the Q-values predicted by the network and the target Q-values.\n",
    "#### Target Net\n",
    "The key idea is that by keeping this network slightly “behind” the policy_net, it reduces the risk of instability in the training process. If only had a single network, the constantly changing Q-values would lead to feedback loops, destabilizing the learning process.\n",
    "- Soft Update: Use a small factor, TAU, to slowly blend the weights from policy_net into target_net, making incremental updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# if GPU, use it\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        # save memory of a transition\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # random sample a batch of transitions\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn trying to predict expected return (Q-value) of taking each possible action given a state (with features decribing the state)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        \"\"\"\n",
    "        n_observations: number of features describing a state of environment\n",
    "            e.g., (position, velocity, angle, angular_velocity), n_observations=4\n",
    "        n_actions: (left, right), n_actions=2\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__() # constructor of parent class\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "gamma = 0.99\n",
    "eps_start = 0.9 # epsilon greedy\n",
    "eps_end = 0.05 # more exploitation in the end\n",
    "eps_decay = 1000 # exponential decay rate of epsilon, higher means slower decay\n",
    "tau = 0.005 # update rate for target network\n",
    "LR = 1e-4 # learning rate\n",
    "n_actions = env.action_space.n # num of actions\n",
    "state, info = env.reset() \n",
    "n_observations = len(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "def select_action(state): # greedy approach\n",
    "    global steps_done\n",
    "    sample = random.random() # uniform random number between 0 and 1\n",
    "    eps_threshold = eps_end + (eps_start - eps_end) * math.exp(-1. * steps_done / eps_decay) # exponential decay eps value\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad(): # no need to compute gradients\n",
    "            \"\"\"\n",
    "            t.max(1) returns largest column value of each row\n",
    "            second column on max result is index of where max element was found\n",
    "            pick action with the largest expected reward (Q-value)\n",
    "            \"\"\"\n",
    "            return policy_net(state).max(1).indices.view(1,1)\n",
    "    else:\n",
    "        # tensor([[0]], device='mps:0')\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "episode_durations = []\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    \"\"\"\n",
    "    performs a single step of the optimization\n",
    "    first samples a batch, compute Q(st, at) and V(st+1)=max_aQ(st+1, a)\n",
    "    then compute the loss and backpropagate\n",
    "    use target network to compute V(st+1) for stability\n",
    "    target network updated at every step with a soft update with tau\n",
    "    \"\"\"\n",
    "    if len(memory) < batch_size:\n",
    "        return # not enough samples to train\n",
    "    \n",
    "    transitions = memory.sample(batch_size)\n",
    "    # transpose the batch, converting batch-array of transitions to transition of batch-arrays\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # check if state is final state (i.e., next state is None), store in a boolean tensor\n",
    "    # mask: tensor of Boolean values, each position indicates whether the corresponding element in another tensor should be selected or ignored\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s:s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    # gather all next-state values not none\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    # concatenate state, action, reward tensors in batch to a single tensor\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # computes Q(s_t, a)\n",
    "    # policy_net generates value for all actions for each state\n",
    "    # .gather(1, action_batch) selects the Q-value for each specific action taken \n",
    "    # (as specified in action_batch) in each state from the output of policy_net(state_batch)\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # computes V(s_t+1)=max_aQ(st+1, a)\n",
    "    # will eventually hold the Q-values of the next_state for each transition in the batch\n",
    "    # terminal states have zero by default\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    with torch.no_grad(): # no need for gradient calculation, only need to calculate target values\n",
    "        \"\"\"\n",
    "        target_net(non_final_next_states): passes the non_final_next_states through \n",
    "            the target network to compute the Q-values for each non-terminal next state\n",
    "        max(1).values finds the maximum Q-value across the action dimension (dim 1, based on Transition namedtuple before)\n",
    "        next_state_values[non_final_mask]: assigns Q-values only to the positions in \n",
    "            next_state_values where non_final_mask is True\n",
    "        \"\"\"\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    # compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    # state_action_values is typically a 2D tensor with a shape like (BATCH_SIZE, 1)\n",
    "    # unsqueeze(1) adds a singleton dimension\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
